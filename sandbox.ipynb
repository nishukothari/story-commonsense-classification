{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tokens import FullTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import statistics\n",
    "from statistics import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(input_string: str) -> list:\n",
    "    res = input_string.strip('][')\n",
    "    res = res.replace('\"', '')\n",
    "    res = res.split(', ')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(row):\n",
    "    for label in row[\"reiss\"]:\n",
    "        if label == 'na':\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "def preprocess(file, bert_layer, only_a_few=False):\n",
    "    file = file[file['action'] != 'no']\n",
    "    file = file[file['motivation'] != 'none']\n",
    "    file = file[file['maslow'] != '[]']\n",
    "    file = file[file['reiss'] != '[]']\n",
    "    file = file[file['reiss'] != '[\"na\"]']\n",
    "    file[\"maslow\"] = file[\"maslow\"].apply(string_to_list)\n",
    "    file[\"reiss\"] = file[\"reiss\"].apply(string_to_list)\n",
    "    file = file[file.apply(my_filter, axis=1)]\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "    mlb_m = MultiLabelBinarizer()\n",
    "    labels_m = mlb_m.fit_transform(file[\"maslow\"])\n",
    "\n",
    "    mlb_r = MultiLabelBinarizer()\n",
    "    labels_r = mlb_r.fit_transform(file[\"reiss\"])\n",
    "\n",
    "    labels = np.concatenate((labels_m, labels_r), axis=1)\n",
    "\n",
    "    data = bert_encode(file['sentence'].values, tokenizer, only_a_few=only_a_few)\n",
    "\n",
    "    if only_a_few:\n",
    "        o_labels = labels[0:32, :]\n",
    "        return data, o_labels\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_motivation = pd.read_csv(os.path.join(\"data/dev/motivation\", \"allcharlinepairs_noids.csv\"))\n",
    "testing_file_motivation = pd.read_csv(os.path.join(\"data/test/motivation\", \"allcharlinepairs_noids.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=128, only_a_few=False):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    j = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        \n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "        j += 1\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=128):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(5, activation='sigmoid')(net)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_labels = preprocess(training_file_motivation, bert_layer)\n",
    "test_input, test_labels = preprocess(testing_file_motivation, bert_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
      "                                 (None, 128, 768)]                'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['keras_layer[0][1]']            \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           49216       ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            165         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,533,702\n",
      "Trainable params: 109,533,701\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/718 [=========>....................] - ETA: 12:17:17 - loss: 2.0168 - accuracy: 0.3302"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, \n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 5)\n",
      "(32, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4074074074074074, 0.2619047619047619, 0.3188405797101449, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = model.predict(test_input)\n",
    "test_output = np.rint(test_output)\n",
    "\n",
    "print(test_output.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "precision_recall_fscore_support(test_labels, test_output, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        #self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_x = tf.Variable(tf.ones(shape = [sequence_length]), dtype=tf.float32)\n",
    "        #self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.input_y = tf.Variable(tf.ones(shape = [num_classes], dtype=tf.float32))\n",
    "        #self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.dropout_keep_prob = tf.Variable(tf.ones(1), dtype = tf.float32)\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        print(args)\n",
    "        self.args = args\n",
    "        \n",
    "        V = args['embed_num']\n",
    "        D = args['embed_dim']\n",
    "        C = args['class_num']\n",
    "        Ci = 1\n",
    "        Co = args['kernel_num']\n",
    "        Ks = args['kernel_sizes']\n",
    "\n",
    "        # V -> number of embeddings -> 128, sentence_size\n",
    "        # D -> embedding dimension -> 768, bert output size\n",
    "        # W\n",
    "\n",
    "        #self.embed = nn.Embedding(V, D)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
    "\n",
    "        if self.args['static']:\n",
    "            self.embed.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)  # (N, W, D)\n",
    "    \n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn(num_classes):\n",
    "    args = {}\n",
    "    args['embed_num'] = 128\n",
    "    args['embed_dim'] = 768\n",
    "    args['class_num'] = num_classes\n",
    "    args['kernel_num'] = 100\n",
    "    args['kernel_sizes'] = [3, 4, 5]\n",
    "    args['dropout'] = 0.5\n",
    "    args['static'] = False\n",
    "    cnn = CNN_Text(\n",
    "        args\n",
    "    )\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_num': 128, 'embed_dim': 768, 'class_num': 6, 'kernel_num': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'static': False}\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [32, 100, 126, 1]         230,500\n",
      "            Conv2d-2          [32, 100, 125, 1]         307,300\n",
      "            Conv2d-3          [32, 100, 124, 1]         384,100\n",
      "           Dropout-4                  [32, 300]               0\n",
      "            Linear-5                    [32, 6]           1,806\n",
      "================================================================\n",
      "Total params: 923,706\n",
      "Trainable params: 923,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 9.23\n",
      "Params size (MB): 3.52\n",
      "Estimated Total Size (MB): 24.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_maslow = build_model_cnn(6)\n",
    "summary(cnn_maslow, (128, 768), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_link = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "bert_preprocessing_link = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(bert_preprocessing_link)\n",
    "bert_model = hub.KerasLayer(bert_model_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(bert_layer, max_len=128):\n",
    "  #text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  #preprocessing_layer = hub.KerasLayer(bert_preprocessing_link, name='preprocessing')\n",
    "  #encoder_inputs = preprocessing_layer(text_input)\n",
    "  #encoder = hub.KerasLayer(bert_model_link, trainable=True, name='BERT_encoder')\n",
    "  #outputs = encoder(encoder_inputs)\n",
    "  #return tf.keras.Model(text_input, outputs)\n",
    "\n",
    "  input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "  segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "  model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output)\n",
    "  model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "bert_raw_model = build_model2(bert_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor: shape=(32,), dtype=string, numpy=\narray([b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b'I let the curry sit before tasting.',\n       b'I let the curry sit before tasting.',\n       b'I let the curry sit before tasting.',\n       b'When it was time to taste, I was disgusted.',\n       b'When it was time to taste, I was disgusted.',\n       b'When it was time to taste, I was disgusted.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'Jervis has been single for a long time.',\n       b'Jervis has been single for a long time.',\n       b'Jervis has been single for a long time.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'They begin to date.', b'They begin to date.'], dtype=object)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/clqv80fs2893t0s91x7ghvk80000gn/T/ipykernel_88138/144175930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msmall_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_file_motivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_raw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\u001b[0m\u001b[1;32m    200\u001b[0m                      \u001b[0;34mf' but it received {len(inputs)} input tensors. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                      f'Inputs received: {inputs}')\n",
      "\u001b[0;31mValueError\u001b[0m: Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor: shape=(32,), dtype=string, numpy=\narray([b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b'I began making fish curry for my boyfriend and I.',\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b\"I decided not to read a recipe since I've made so many in my life.\",\n       b'I let the curry sit before tasting.',\n       b'I let the curry sit before tasting.',\n       b'I let the curry sit before tasting.',\n       b'When it was time to taste, I was disgusted.',\n       b'When it was time to taste, I was disgusted.',\n       b'When it was time to taste, I was disgusted.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'I accidentally used a whole garlic instead of a whole onion.',\n       b'Jervis has been single for a long time.',\n       b'Jervis has been single for a long time.',\n       b'Jervis has been single for a long time.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'He wants to have a girlfriend.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'One day he meets a nice girl at the grocery store.',\n       b'They begin to date.', b'They begin to date.'], dtype=object)>]"
     ]
    }
   ],
   "source": [
    "small_test = training_file_motivation[0:32][\"sentence\"].values\n",
    "out = bert_raw_model(small_test)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 768)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(out['sequence_output'].shape)\n",
    "print(type(out[\"sequence_output\"]))\n",
    "# Batch Size, Sentence Size, Embedding Size\n",
    "# 32\n",
    "# 128\n",
    "# 768\n",
    "npout = out[\"sequence_output\"].numpy()\n",
    "torchout = torch.from_numpy(npout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_num': 128, 'embed_dim': 768, 'class_num': 5, 'kernel_num': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'static': False}\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args['embed_num'] = 128\n",
    "args['embed_dim'] = 768\n",
    "args['class_num'] = 5\n",
    "args['kernel_num'] = 100\n",
    "args['kernel_sizes'] = [3, 4, 5]\n",
    "args['dropout'] = 0.5\n",
    "args['static'] = False\n",
    "cnn = CNN_Text(\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 768])\n",
      "tensor([[ 9.6613e-02, -3.8865e-02, -2.8047e-01,  ..., -4.9758e-01,\n",
      "          4.7888e-01,  3.5955e-01],\n",
      "        [ 5.8334e-01, -2.1532e-01, -4.6476e-01,  ..., -1.9645e-01,\n",
      "          8.5647e-01,  7.0585e-02],\n",
      "        [-2.5741e-01,  3.2241e-01, -1.0095e+00,  ..., -1.0464e-01,\n",
      "          4.4049e-01, -9.8860e-01],\n",
      "        ...,\n",
      "        [ 5.2097e-01, -5.9698e-02,  2.5945e-01,  ...,  8.6826e-04,\n",
      "          6.0448e-02, -2.0960e-01],\n",
      "        [ 2.1364e-01, -2.0435e-01,  6.2357e-02,  ...,  2.0564e-01,\n",
      "          2.1234e-01, -4.5383e-01],\n",
      "        [ 3.6563e-01,  9.4640e-02,  3.0396e-01,  ..., -1.6612e-02,\n",
      "         -5.0270e-02, -2.3940e-01]])\n",
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torchout.shape)\n",
    "print(torchout[0])\n",
    "model_out = cnn.forward(torchout)\n",
    "\n",
    "print(model_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:870: UserWarning: unknown class(es) ['gong'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb1 = MultiLabelBinarizer()\n",
    "mlb1.fit([[\"maslow\"], [\"reiss\"], [\"plutchik\"]])\n",
    "out = mlb1.transform([\n",
    "    [\"maslow\"], [\"reiss\", \"gong\"]\n",
    "])\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storyid</th>\n",
       "      <th>linenum</th>\n",
       "      <th>char</th>\n",
       "      <th>context</th>\n",
       "      <th>sentence</th>\n",
       "      <th>action</th>\n",
       "      <th>motivation</th>\n",
       "      <th>maslow</th>\n",
       "      <th>reiss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>eat some food</td>\n",
       "      <td>[physiological]</td>\n",
       "      <td>[\"food\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>to be nice</td>\n",
       "      <td>[love, physiological]</td>\n",
       "      <td>[\"family\", \"romance\", \"food\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>to satisfy a craving.</td>\n",
       "      <td>[physiological]</td>\n",
       "      <td>[\"food\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>Boyfriend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>no</td>\n",
       "      <td>[\"none\"]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[\"none\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>yes</td>\n",
       "      <td>to help out</td>\n",
       "      <td>[spiritual growth]</td>\n",
       "      <td>[\"indep\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>yes</td>\n",
       "      <td>confidence</td>\n",
       "      <td>[physiological]</td>\n",
       "      <td>[\"food\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>yes</td>\n",
       "      <td>to prove knowledge</td>\n",
       "      <td>[spiritual growth]</td>\n",
       "      <td>[\"indep\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>Boyfriend</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>no</td>\n",
       "      <td>[\"none\"]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[\"none\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>3</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>I began making fish curry for my boyfriend and...</td>\n",
       "      <td>I let the curry sit before tasting.</td>\n",
       "      <td>yes</td>\n",
       "      <td>cook good food</td>\n",
       "      <td>[esteem, love]</td>\n",
       "      <td>[\"competition\", \"approval\", \"romance\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>3</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>I began making fish curry for my boyfriend and...</td>\n",
       "      <td>I let the curry sit before tasting.</td>\n",
       "      <td>yes</td>\n",
       "      <td>to trust myself</td>\n",
       "      <td>[spiritual growth, esteem]</td>\n",
       "      <td>[\"indep\", \"competition\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                storyid  linenum        char  \\\n",
       "0  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "1  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "2  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "3  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1   Boyfriend   \n",
       "4  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2  I (myself)   \n",
       "5  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2  I (myself)   \n",
       "6  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2  I (myself)   \n",
       "7  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2   Boyfriend   \n",
       "8  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        3  I (myself)   \n",
       "9  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        3  I (myself)   \n",
       "\n",
       "                                             context  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  I began making fish curry for my boyfriend and I.   \n",
       "5  I began making fish curry for my boyfriend and I.   \n",
       "6  I began making fish curry for my boyfriend and I.   \n",
       "7  I began making fish curry for my boyfriend and I.   \n",
       "8  I began making fish curry for my boyfriend and...   \n",
       "9  I began making fish curry for my boyfriend and...   \n",
       "\n",
       "                                            sentence action  \\\n",
       "0  I began making fish curry for my boyfriend and I.    yes   \n",
       "1  I began making fish curry for my boyfriend and I.    yes   \n",
       "2  I began making fish curry for my boyfriend and I.    yes   \n",
       "3  I began making fish curry for my boyfriend and I.     no   \n",
       "4  I decided not to read a recipe since I've made...    yes   \n",
       "5  I decided not to read a recipe since I've made...    yes   \n",
       "6  I decided not to read a recipe since I've made...    yes   \n",
       "7  I decided not to read a recipe since I've made...     no   \n",
       "8                I let the curry sit before tasting.    yes   \n",
       "9                I let the curry sit before tasting.    yes   \n",
       "\n",
       "              motivation                      maslow  \\\n",
       "0          eat some food             [physiological]   \n",
       "1             to be nice       [love, physiological]   \n",
       "2  to satisfy a craving.             [physiological]   \n",
       "3               [\"none\"]                      [none]   \n",
       "4            to help out          [spiritual growth]   \n",
       "5             confidence             [physiological]   \n",
       "6     to prove knowledge          [spiritual growth]   \n",
       "7               [\"none\"]                      [none]   \n",
       "8         cook good food              [esteem, love]   \n",
       "9        to trust myself  [spiritual growth, esteem]   \n",
       "\n",
       "                                    reiss  \n",
       "0                                [\"food\"]  \n",
       "1           [\"family\", \"romance\", \"food\"]  \n",
       "2                                [\"food\"]  \n",
       "3                                [\"none\"]  \n",
       "4                               [\"indep\"]  \n",
       "5                                [\"food\"]  \n",
       "6                               [\"indep\"]  \n",
       "7                                [\"none\"]  \n",
       "8  [\"competition\", \"approval\", \"romance\"]  \n",
       "9                [\"indep\", \"competition\"]  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file_motivation.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_maslow(df):\n",
    "    all_maslow = df.values.sum()\n",
    "    most_freq = mode(all_maslow)\n",
    "    if most_freq == '':\n",
    "        most_freq = 'none'\n",
    "    return [most_freq]\n",
    "\n",
    "def combine_reiss(df):\n",
    "    all_reiss = df.values.sum()\n",
    "    most_freq = mode(all_reiss)\n",
    "    if most_freq == 'na' or most_freq == '':\n",
    "        most_freq = 'none'\n",
    "    return [most_freq]\n",
    "\n",
    "def combine_sentence(df):\n",
    "    full_sentence = []\n",
    "    full_sentence.append([df['char']])\n",
    "    full_sentence.append([df['sentence']])\n",
    "    \n",
    "    if not pd.isnull(df['context']):\n",
    "        split_context = df['context'].split(\"|\")\n",
    "        i = len(split_context) - 1\n",
    "        while i >= 0:\n",
    "            full_sentence.append([split_context[i]])\n",
    "            i -= 1\n",
    "\n",
    "    return full_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(training_file_motivation['context'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_motivation[\"maslow\"] = training_file_motivation[\"maslow\"].apply(string_to_list)\n",
    "training_file_motivation[\"reiss\"] = training_file_motivation[\"reiss\"].apply(string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storyid</th>\n",
       "      <th>linenum</th>\n",
       "      <th>char</th>\n",
       "      <th>maslow</th>\n",
       "      <th>reiss</th>\n",
       "      <th>full_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[[My daughter], [Nana came into the room with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>1</td>\n",
       "      <td>Nana</td>\n",
       "      <td>[stability]</td>\n",
       "      <td>[order]</td>\n",
       "      <td>[[Nana], [Nana came into the room with a puzzl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>2</td>\n",
       "      <td>My daughter</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[[My daughter], [She held up an orange sock an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>2</td>\n",
       "      <td>Nana</td>\n",
       "      <td>[stability]</td>\n",
       "      <td>[order]</td>\n",
       "      <td>[[Nana], [She held up an orange sock and a blu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>3</td>\n",
       "      <td>My daughter</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[[My daughter], [My daughter jumped up and gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>3</td>\n",
       "      <td>Nana</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[[Nana], [My daughter jumped up and grabbed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>4</td>\n",
       "      <td>My daughter</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[family]</td>\n",
       "      <td>[[My daughter], [She took off running down the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>4</td>\n",
       "      <td>Nana</td>\n",
       "      <td>[esteem]</td>\n",
       "      <td>[status]</td>\n",
       "      <td>[[Nana], [She took off running down the hall w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>5</td>\n",
       "      <td>My daughter</td>\n",
       "      <td>[spiritual growth]</td>\n",
       "      <td>[serenity]</td>\n",
       "      <td>[[My daughter], [Nana chased her down, caught ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0008c800-82b6-43b3-8b53-5475ed1dac9b</td>\n",
       "      <td>5</td>\n",
       "      <td>Nana</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[family]</td>\n",
       "      <td>[[Nana], [Nana chased her down, caught her, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                storyid  linenum         char  \\\n",
       "0  0008c800-82b6-43b3-8b53-5475ed1dac9b        1  My daughter   \n",
       "1  0008c800-82b6-43b3-8b53-5475ed1dac9b        1         Nana   \n",
       "2  0008c800-82b6-43b3-8b53-5475ed1dac9b        2  My daughter   \n",
       "3  0008c800-82b6-43b3-8b53-5475ed1dac9b        2         Nana   \n",
       "4  0008c800-82b6-43b3-8b53-5475ed1dac9b        3  My daughter   \n",
       "5  0008c800-82b6-43b3-8b53-5475ed1dac9b        3         Nana   \n",
       "6  0008c800-82b6-43b3-8b53-5475ed1dac9b        4  My daughter   \n",
       "7  0008c800-82b6-43b3-8b53-5475ed1dac9b        4         Nana   \n",
       "8  0008c800-82b6-43b3-8b53-5475ed1dac9b        5  My daughter   \n",
       "9  0008c800-82b6-43b3-8b53-5475ed1dac9b        5         Nana   \n",
       "\n",
       "               maslow       reiss  \\\n",
       "0              [none]      [none]   \n",
       "1         [stability]     [order]   \n",
       "2              [none]      [none]   \n",
       "3         [stability]     [order]   \n",
       "4              [none]      [none]   \n",
       "5              [none]      [none]   \n",
       "6              [love]    [family]   \n",
       "7            [esteem]    [status]   \n",
       "8  [spiritual growth]  [serenity]   \n",
       "9              [love]    [family]   \n",
       "\n",
       "                                       full_sentence  \n",
       "0  [[My daughter], [Nana came into the room with ...  \n",
       "1  [[Nana], [Nana came into the room with a puzzl...  \n",
       "2  [[My daughter], [She held up an orange sock an...  \n",
       "3  [[Nana], [She held up an orange sock and a blu...  \n",
       "4  [[My daughter], [My daughter jumped up and gra...  \n",
       "5  [[Nana], [My daughter jumped up and grabbed th...  \n",
       "6  [[My daughter], [She took off running down the...  \n",
       "7  [[Nana], [She took off running down the hall w...  \n",
       "8  [[My daughter], [Nana chased her down, caught ...  \n",
       "9  [[Nana], [Nana chased her down, caught her, an...  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file_motivation['full_sentence'] = training_file_motivation.apply(combine_sentence, axis=1)\n",
    "groups_motivation = training_file_motivation.groupby(['storyid', 'linenum', 'char']).aggregate({\n",
    "    'maslow': lambda x: combine_maslow(x),\n",
    "    'reiss':  lambda x: combine_reiss(x),\n",
    "    'full_sentence': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "groups_motivation.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = []\n",
    "input_seq = [\"a\"] + [\"b\"]\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = np.array([[0, 0, 1], [0, 1, 0]])\n",
    "normal_labels = np.argmax(one_hot, axis=1)\n",
    "normal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 1]\n",
      "  [0 0 1]]\n",
      "\n",
      " [[0 1 0]\n",
      "  [1 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [[1, 2, 3], [2, 3, 4]],\n",
    "    [[1, 5, 3], [5, 3, 4]]\n",
    "])\n",
    "\n",
    "def predict_one_hot(x):\n",
    "    d = x.reshape(-1, x.shape[-1])\n",
    "    d2 = np.zeros_like(d)\n",
    "    d2[np.arange(len(d2)), d.argmax(1)] = 1\n",
    "    d2 = d2.reshape(x.shape)\n",
    "    return d2\n",
    "\n",
    "x = np.apply_along_axis(predict_one_hot, axis=2, arr=x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8569915294647217, 2.648024082183838, 2.627460479736328, 2.5431251525878906, 2.6502315998077393, 2.559394121170044, 2.5106284618377686, 2.4026851654052734, 2.553509473800659, 2.4749064445495605, 2.4849624633789062, 2.5010833740234375, 2.436309337615967, 2.478527307510376, 2.3792240619659424, 2.678799629211426, 2.4531025886535645, 2.5425326824188232, 2.3759524822235107, 2.3523106575012207, 2.6161282062530518, 2.5512378215789795, 2.4129180908203125, 2.5047948360443115, 2.5327351093292236, 2.4347705841064453, 2.385363817214966, 2.4757323265075684, 2.4340829849243164, 2.5617778301239014, 2.5291104316711426, 2.438674211502075, 2.4606833457946777, 2.402775287628174, 2.4225549697875977, 2.4496407508850098, 2.565316677093506, 2.2991344928741455, 2.5334858894348145, 2.416416645050049, 2.536299467086792, 2.5642588138580322, 2.4858572483062744, 2.4053430557250977, 2.5981903076171875, 2.3955605030059814, 2.613661527633667, 2.4025635719299316, 2.362304210662842, 2.36513614654541, 2.349695920944214, 2.4850428104400635, 2.4285364151000977, 2.4378249645233154, 2.483024835586548, 2.4807369709014893, 2.685436725616455, 2.4578857421875, 2.647355079650879, 2.558424234390259, 2.44698166847229, 2.505779504776001, 2.5281245708465576, 2.2782485485076904, 2.3464887142181396, 2.409820318222046, 2.3667056560516357, 2.520979642868042, 2.4829185009002686, 2.4196038246154785, 2.4138622283935547, 2.2856125831604004, 2.5466532707214355, 2.562138795852661, 2.3217546939849854, 2.366009473800659, 2.4049525260925293, 2.6829071044921875, 2.4608471393585205, 2.583569288253784, 2.4081227779388428, 2.5103981494903564, 2.5870423316955566, 2.6156020164489746, 2.2760660648345947, 2.579355001449585, 2.4711339473724365, 2.583051919937134, 2.4595627784729004, 2.525867223739624, 2.439897060394287, 2.370790719985962, 2.531001091003418, 2.4439587593078613, 2.4432029724121094, 2.531871795654297, 2.4985055923461914, 2.4986369609832764, 2.524221658706665, 2.37412691116333, 2.484428644180298, 2.378486394882202, 2.394977331161499, 2.5040156841278076, 2.4184792041778564, 2.4018282890319824, 2.328839063644409, 2.549389600753784, 2.486461639404297, 2.487818479537964, 2.4506711959838867, 2.5420000553131104, 2.5885350704193115, 2.3124659061431885, 2.5986666679382324, 2.5640878677368164, 2.4354331493377686, 2.576173782348633, 2.4205169677734375, 2.5972049236297607, 2.607239007949829, 2.424121141433716, 2.481325626373291, 2.538370370864868, 2.4912326335906982, 2.346442937850952, 2.4677867889404297, 2.499005079269409, 2.4356985092163086, 2.566200017929077, 2.5855588912963867, 2.5229380130767822, 2.478973150253296, 2.5279769897460938, 2.4353604316711426, 2.438249111175537, 2.5085272789001465, 2.5526130199432373, 2.4949586391448975, 2.428312063217163, 2.5300025939941406, 2.336899518966675, 2.5142579078674316, 2.405437469482422, 2.5218920707702637, 2.398660659790039, 2.507692813873291, 2.4234254360198975, 2.3769214153289795, 2.492037296295166, 2.4552323818206787, 2.460470199584961, 2.458400011062622, 2.491194248199463, 2.432682991027832, 2.5581953525543213, 2.517652750015259, 2.4574129581451416, 2.5464534759521484, 2.477748394012451, 2.443495988845825, 2.5546631813049316, 2.513296604156494, 2.5807371139526367, 2.396879196166992, 2.4306869506835938, 2.27131724357605, 2.47499418258667, 2.3491199016571045, 2.4286272525787354, 2.4695000648498535, 2.4489426612854004, 2.4466772079467773, 2.3679494857788086, 2.338439702987671, 2.4770359992980957, 2.535222053527832, 2.31673002243042, 2.2921128273010254, 2.4852256774902344, 2.403592586517334, 2.6583495140075684, 2.3189456462860107, 2.3872458934783936, 2.5831408500671387, 2.490316152572632, 2.5180559158325195, 2.3990678787231445, 2.54951548576355, 2.492825746536255, 2.327364921569824, 2.5419182777404785, 2.421783924102783, 2.5448455810546875, 2.488795042037964, 2.455519914627075, 2.414748430252075, 2.386446714401245, 2.5038557052612305, 2.423839807510376, 2.5218398571014404, 2.561551094055176, 2.5012073516845703, 2.5543863773345947, 2.412841320037842, 2.396439552307129, 2.3693039417266846, 2.4206442832946777, 2.504427909851074, 2.512798547744751, 2.4158761501312256, 2.4305503368377686, 2.517807960510254, 2.490450382232666, 2.450087308883667, 2.3308115005493164, 2.527684211730957, 2.4013166427612305, 2.456547975540161, 2.5298502445220947, 2.4553182125091553, 2.424325466156006, 2.511482000350952, 2.3309874534606934, 2.473968505859375, 2.334195375442505, 2.411363124847412, 2.533449411392212, 2.5742342472076416, 2.5675432682037354, 2.4589879512786865, 2.501363754272461, 2.482097864151001, 2.4885149002075195, 2.193260669708252, 2.488680601119995, 2.4247374534606934, 2.458852767944336, 2.540158271789551, 2.58339786529541, 2.6585004329681396, 2.440887689590454, 2.5487236976623535, 2.447740077972412, 2.4701716899871826, 2.3755130767822266, 2.45632266998291, 2.537642002105713, 2.323545217514038, 2.4016098976135254, 2.5444605350494385, 2.563859701156616, 2.433553457260132, 2.4261951446533203, 2.5467798709869385, 2.4838109016418457, 2.397397518157959, 2.438425064086914, 2.605260133743286, 2.6075568199157715, 2.5682382583618164, 2.3758373260498047, 2.635481834411621, 2.4978697299957275, 2.3992373943328857, 2.5884528160095215, 2.4323365688323975, 2.3306703567504883, 2.579216241836548, 2.4454517364501953, 2.4674506187438965, 2.4962902069091797, 2.5122263431549072, 2.460599422454834, 2.543928623199463, 2.4214589595794678, 2.440643310546875, 2.416715145111084, 2.3687450885772705, 2.588608503341675, 2.4329588413238525, 2.4428412914276123, 2.553753137588501, 2.5624053478240967, 2.387666702270508, 2.4041852951049805, 2.532015323638916, 2.4708170890808105, 2.5024914741516113, 2.5297000408172607, 2.4955458641052246, 2.3172900676727295, 2.4664676189422607, 2.469430923461914, 2.4716947078704834, 2.541577100753784, 2.373861312866211, 2.5033254623413086, 2.3746869564056396, 2.3452043533325195, 2.4962565898895264, 2.3508896827697754, 2.5504934787750244, 2.3423359394073486, 2.5932986736297607, 2.532904624938965, 2.562955617904663, 2.567493438720703, 2.5188820362091064, 2.5570173263549805, 2.3513145446777344, 2.4646689891815186, 2.464439868927002, 2.517604112625122, 2.407682180404663, 2.439852476119995, 2.439824342727661, 2.4641547203063965, 2.6868674755096436, 2.5248525142669678, 2.5269198417663574, 2.431194305419922, 2.428781509399414, 2.530402421951294, 2.3869516849517822, 2.3812904357910156, 2.347484827041626, 2.400047540664673, 2.5338168144226074, 2.4276437759399414, 2.409651041030884, 2.505861520767212, 2.4938533306121826, 2.5012176036834717, 2.517406940460205, 2.306218385696411, 2.492478609085083, 2.255824327468872, 2.377622365951538, 2.4527618885040283, 2.598093032836914, 2.460205078125, 2.611872434616089, 2.5825533866882324, 2.5594687461853027, 2.546189546585083, 2.4878008365631104, 2.4294018745422363, 2.5415985584259033, 2.3036274909973145, 2.425855875015259, 2.637737989425659, 2.3419761657714844, 2.5161702632904053, 2.553586006164551, 2.6896398067474365, 2.5852408409118652, 2.4622297286987305, 2.39768648147583, 2.43593692779541, 2.3300673961639404, 2.3895814418792725, 2.3470757007598877, 2.4059131145477295, 2.4159462451934814, 2.485560417175293, 2.5161666870117188, 2.342409610748291, 2.4261159896850586, 2.5312561988830566, 2.4301791191101074, 2.336482286453247, 2.4037842750549316, 2.5562620162963867, 2.521573543548584, 2.3429107666015625, 2.5215251445770264, 2.498943567276001, 2.515723466873169, 2.433788537979126, 2.605349540710449, 2.532224655151367, 2.5390102863311768, 2.3958773612976074, 2.428068161010742, 2.616544008255005, 2.4929263591766357, 2.5030832290649414, 2.474273920059204, 2.5319197177886963, 2.5372519493103027, 2.497817039489746, 2.3751704692840576, 2.4078729152679443, 2.559847831726074, 2.437668561935425, 2.404045581817627, 2.555663824081421, 2.3989007472991943, 2.439164161682129, 2.404902458190918, 2.3416409492492676, 2.4588205814361572, 2.543311834335327, 2.4365549087524414, 2.6014347076416016, 2.3360514640808105, 2.4177358150482178, 2.365968942642212, 2.415754556655884, 2.381303071975708, 2.531142234802246, 2.3411014080047607, 2.58695912361145, 2.3775148391723633, 2.5581653118133545, 2.458237886428833, 2.4102070331573486, 2.456425905227661, 2.367297410964966, 2.6010031700134277, 2.61055588722229, 2.44439959526062, 2.665963649749756, 2.5618772506713867, 2.3139233589172363, 2.288301706314087, 2.3862690925598145, 2.3545806407928467, 2.518042802810669, 2.634809970855713, 2.5292303562164307, 2.511970043182373, 2.563845157623291, 2.561826229095459, 2.516695499420166, 2.4650378227233887, 2.6727333068847656, 2.541992664337158, 2.4461023807525635, 2.479774236679077, 2.4700875282287598, 2.4997687339782715, 2.327169418334961, 2.5161285400390625, 2.3367207050323486, 2.5048959255218506, 2.4063026905059814, 2.5947165489196777, 2.5493457317352295, 2.2163991928100586, 2.5768237113952637, 2.5355992317199707, 2.3417961597442627, 2.494793653488159, 2.5369415283203125, 2.5437333583831787, 2.5296924114227295, 2.4037797451019287, 2.507715940475464, 2.3474910259246826, 2.5536081790924072, 2.5243330001831055, 2.4368972778320312, 2.503221035003662, 2.471196413040161, 2.45835280418396, 2.45157527923584, 2.5290443897247314, 2.3421335220336914, 2.634490728378296, 2.5872151851654053, 2.459212064743042, 2.5605850219726562, 2.484212875366211, 2.424750328063965, 2.432865619659424, 2.4344842433929443, 2.5458567142486572, 2.5970661640167236, 2.4884958267211914, 2.458357572555542, 2.4551174640655518, 2.4297409057617188, 2.5590341091156006, 2.4612221717834473, 2.5136451721191406, 2.426898241043091, 2.325843572616577, 2.5624454021453857, 2.505157947540283, 2.453630208969116, 2.5275659561157227, 2.6027987003326416, 2.4726552963256836, 2.4786412715911865, 2.509535789489746, 2.6148641109466553, 2.5318477153778076, 2.6033883094787598, 2.389772891998291, 2.4940967559814453, 2.5638632774353027, 2.228585720062256, 2.5235555171966553, 2.3338165283203125, 2.62308669090271, 2.5549416542053223, 2.3490142822265625, 2.4977939128875732, 2.4415879249572754, 2.44328236579895, 2.363114833831787, 2.4807612895965576, 2.576660633087158, 2.5025901794433594, 2.5068256855010986, 2.3585331439971924, 2.6255340576171875, 2.4765994548797607, 2.522797107696533, 2.4826178550720215, 2.481309652328491, 2.471102476119995, 2.4829204082489014, 2.590453863143921, 2.359769582748413, 2.3473639488220215, 2.527198076248169, 2.506256580352783, 2.4539194107055664, 2.6100270748138428, 2.369218587875366, 2.4726908206939697, 2.3863041400909424, 2.6720590591430664, 2.314748764038086, 2.3954362869262695, 2.5929861068725586, 2.6409215927124023, 2.435119152069092, 2.401984691619873, 2.433133363723755, 2.504204511642456, 2.6208090782165527, 2.5522544384002686, 2.4156100749969482, 2.4518826007843018, 2.37121844291687, 2.4183952808380127, 2.489459991455078, 2.4560015201568604, 2.5743985176086426, 2.644407033920288, 2.40627384185791, 2.5280895233154297, 2.4400739669799805, 2.5319900512695312, 2.492198944091797, 2.440049648284912, 2.5117106437683105, 2.4645779132843018, 2.3136041164398193, 2.348870277404785, 2.378554582595825, 2.6124796867370605, 2.5353469848632812, 2.5096256732940674, 2.565178394317627, 2.420055389404297, 2.4315614700317383, 2.54540753364563, 2.4830739498138428, 2.3753538131713867, 2.479396104812622, 2.622926712036133, 2.3236329555511475, 2.470520496368408, 2.288238525390625, 2.4495787620544434, 2.575819253921509, 2.513331174850464, 2.4495420455932617, 2.493298053741455, 2.619863510131836, 2.440697431564331, 2.512017011642456, 2.5242886543273926, 2.557316541671753, 2.500396490097046, 2.6479594707489014, 2.5014278888702393, 2.4715988636016846, 2.6392672061920166, 2.429036855697632, 2.356825351715088, 2.5404341220855713, 2.5270113945007324, 2.515455484390259, 2.4270412921905518, 2.5903680324554443, 2.3365530967712402, 2.416822671890259, 2.603698968887329, 2.353050947189331, 2.3253791332244873, 2.5294876098632812, 2.374595880508423, 2.4678921699523926, 2.4097952842712402, 2.4334604740142822, 2.4764325618743896, 2.4994449615478516, 2.535494565963745, 2.414634943008423, 2.322211742401123, 2.492680072784424, 2.4751620292663574, 2.4047157764434814, 2.5382189750671387, 2.3162777423858643, 2.438302993774414, 2.4140031337738037, 2.410126209259033, 2.467061758041382, 2.5329396724700928, 2.458073139190674, 2.6104681491851807, 2.440736770629883, 2.5416197776794434, 2.428161144256592, 2.340902805328369, 2.3249094486236572, 2.5922889709472656, 2.5747430324554443, 2.4137184619903564, 2.639826536178589, 2.357262372970581, 2.3838772773742676, 2.5247397422790527, 2.347313642501831, 2.666914939880371, 2.6400647163391113, 2.372145652770996, 2.4221789836883545, 2.4601707458496094, 2.615272045135498, 2.426757335662842, 2.487260341644287, 2.5256316661834717, 2.365818500518799, 2.450131893157959, 2.4618313312530518, 2.648501396179199, 2.5249991416931152, 2.4554295539855957, 2.3720743656158447, 2.613450288772583, 2.448686361312866, 2.4487898349761963, 2.5308001041412354, 2.416189432144165, 2.331033706665039, 2.325660228729248, 2.4303832054138184, 2.435889482498169, 2.362287759780884, 2.4503583908081055, 2.4488954544067383, 2.5517749786376953, 2.7457194328308105, 2.4103810787200928, 2.458930015563965, 2.4569737911224365, 2.4909064769744873, 2.4936611652374268, 2.4276297092437744, 2.555525064468384, 2.4614412784576416, 2.422471284866333, 2.5797111988067627, 2.3219707012176514, 2.5820295810699463, 2.4111130237579346, 2.474762439727783, 2.50886607170105, 2.521397829055786, 2.320996046066284, 2.5363945960998535, 2.573087215423584, 2.377998113632202, 2.5837132930755615, 2.4185752868652344, 2.455552101135254, 2.5028235912323, 2.3822290897369385, 2.2866370677948, 2.603868246078491, 2.4605844020843506, 2.450061082839966, 2.504573106765747, 2.4955027103424072, 2.5129449367523193, 2.5464587211608887, 2.511136531829834, 2.537696123123169, 2.4434127807617188, 2.379405975341797, 2.3517215251922607, 2.294468879699707, 2.408846616744995, 2.407116651535034, 2.443598508834839, 2.4727344512939453, 2.353619337081909, 2.567533016204834, 2.340620994567871, 2.505009889602661, 2.4085447788238525, 2.4255874156951904, 2.442941665649414, 2.4988067150115967, 2.416114330291748, 2.495398998260498, 2.4033164978027344, 2.4910881519317627, 2.468210220336914, 2.4413607120513916, 2.4176721572875977, 2.538128137588501, 2.3445892333984375, 2.578738212585449, 2.58608078956604, 2.4496002197265625, 2.4760944843292236, 2.4194157123565674, 2.4421749114990234, 2.2991108894348145, 2.441495656967163, 2.48009991645813, 2.3155648708343506, 2.589292526245117, 2.542480230331421, 2.402339458465576, 2.518707752227783, 2.519430637359619, 2.3989052772521973, 2.4043691158294678, 2.518871307373047, 2.5922865867614746, 2.5026164054870605, 2.463198661804199, 2.4655416011810303, 2.5581185817718506, 2.549229860305786, 2.396400213241577, 2.4319586753845215, 2.518946886062622, 2.555454730987549, 2.4276490211486816, 2.438215494155884, 2.3775506019592285, 2.4087419509887695, 2.620328187942505, 2.4339168071746826, 2.396239995956421, 2.4607465267181396, 2.2614376544952393, 2.524731397628784, 2.520509719848633, 2.4720823764801025, 2.553441047668457, 2.5552382469177246, 2.400874614715576, 2.4141390323638916, 2.3382785320281982, 2.4908194541931152, 2.4778506755828857, 2.4797823429107666, 2.3815300464630127, 2.5125510692596436, 2.4827780723571777, 2.4567341804504395, 2.5338311195373535, 2.3594977855682373, 2.345588207244873, 2.410975217819214, 2.558683156967163, 2.5352678298950195, 2.5312891006469727, 2.4049246311187744, 2.5684611797332764, 2.4019460678100586, 2.4493844509124756, 2.5867655277252197, 2.5318551063537598, 2.5613694190979004, 2.7052178382873535, 2.4128060340881348, 2.5801312923431396, 2.515164375305176, 2.3844010829925537, 2.600768804550171, 2.225867748260498, 2.2614407539367676, 2.5443618297576904, 2.497638702392578, 2.4394116401672363, 2.660409450531006, 2.595115900039673, 2.5781145095825195, 2.4015467166900635, 2.4207921028137207, 2.422619581222534, 2.563786029815674, 2.3973166942596436, 2.406069040298462, 2.5228564739227295, 2.435673713684082, 2.4560298919677734, 2.4220457077026367, 2.376323699951172, 2.455298900604248, 2.554103136062622, 2.4653806686401367, 2.489492416381836, 2.2426371574401855, 2.396254777908325, 2.435856342315674, 2.3392574787139893, 2.45631742477417, 2.4273722171783447, 2.3734147548675537, 2.4340226650238037, 2.38267183303833, 2.4314792156219482, 2.553938150405884, 2.4826202392578125, 2.523566484451294, 2.373328924179077, 2.4955224990844727, 2.4797255992889404, 2.4137771129608154, 2.341736078262329, 2.4146203994750977, 2.2944605350494385, 2.4815573692321777, 2.318809986114502, 2.511845588684082, 2.4432504177093506, 2.453296422958374, 2.4000394344329834, 2.447254180908203, 2.5207104682922363, 2.485605239868164, 2.410585880279541, 2.481109142303467, 2.5397093296051025, 2.5387632846832275, 2.4718732833862305, 2.420145273208618, 2.4491066932678223, 2.3820033073425293, 2.68109130859375, 2.569863796234131, 2.6375861167907715, 2.5554418563842773, 2.483994483947754, 2.4268734455108643, 2.3921446800231934, 2.4065215587615967, 2.5163872241973877, 2.394047260284424, 2.362436056137085, 2.4900498390197754, 2.398954391479492, 2.323183298110962, 2.319091320037842, 2.551089286804199, 2.5485434532165527, 2.741570234298706, 2.616624355316162, 2.2497971057891846, 2.428709030151367, 2.6114039421081543, 2.4893453121185303, 2.300004482269287, 2.399524450302124, 2.432314395904541, 2.5249898433685303, 2.553176164627075, 2.5129032135009766, 2.333134412765503, 2.617849111557007, 2.4513132572174072, 2.3111636638641357, 2.4668917655944824, 2.542325973510742, 2.328375816345215, 2.4446194171905518, 2.583803653717041, 2.579861879348755, 2.3824219703674316, 2.4772181510925293, 2.3912272453308105, 2.4887471199035645, 2.6058766841888428, 2.267453908920288, 2.459739923477173, 2.5538418292999268, 2.4576799869537354, 2.4472358226776123, 2.420045852661133, 2.637317419052124, 2.5354883670806885, 2.602339506149292, 2.339246988296509, 2.457927703857422, 2.4039034843444824, 2.430952548980713, 2.468717336654663, 2.5438499450683594, 2.3622689247131348, 2.486166477203369, 2.435197353363037, 2.6931240558624268, 2.2690958976745605, 2.249464750289917, 2.4771971702575684, 2.5133721828460693, 2.48490047454834, 2.5351321697235107, 2.414288282394409, 2.676194667816162, 2.58333158493042, 2.418154239654541, 2.4314324855804443, 2.4660556316375732, 2.6862053871154785, 2.462514638900757, 2.5631306171417236, 2.432642698287964, 2.54610013961792, 2.5931577682495117, 2.2855165004730225, 2.4191155433654785, 2.5505716800689697, 2.5224926471710205, 2.338815689086914, 2.430659294128418, 2.481785297393799, 2.347214460372925, 2.5807065963745117, 2.409332036972046, 2.5616860389709473, 2.427656650543213, 2.482059955596924, 2.473085641860962, 2.4319534301757812, 2.481506824493408, 2.5706863403320312, 2.4911797046661377, 2.6601037979125977, 2.5763397216796875, 2.3776326179504395, 2.427236795425415, 2.4222140312194824, 2.317045211791992, 2.347032308578491, 2.3581576347351074, 2.497586965560913, 2.437697649002075, 2.488192319869995, 2.5866456031799316, 2.6589910984039307, 2.5098466873168945, 2.477113723754883, 2.473717212677002, 2.5667436122894287, 2.3710718154907227, 2.3325448036193848, 2.6586813926696777, 2.4379565715789795, 2.255620241165161, 2.5001180171966553, 2.609287738800049, 2.4058971405029297, 2.4816434383392334, 2.2902567386627197, 2.4376914501190186, 2.4591333866119385, 2.60187029838562, 2.465303897857666, 2.6649227142333984, 2.2779417037963867, 2.48606538772583, 2.3298306465148926, 2.6291239261627197, 2.400357723236084, 2.3602209091186523, 2.3213095664978027, 2.6027135848999023, 2.450615406036377, 2.342360019683838, 2.607308864593506, 2.5375399589538574, 2.467500686645508, 2.551281690597534, 2.5708155632019043, 2.4423656463623047, 2.4656577110290527, 2.49468731880188, 2.5283284187316895, 2.4293344020843506, 2.4291255474090576, 2.520766496658325, 2.5545754432678223, 2.583714008331299, 2.4290525913238525, 2.3043577671051025, 2.41070556640625, 2.4885759353637695, 2.467414617538452, 2.478196382522583, 2.3634886741638184, 2.470215320587158, 2.4806528091430664, 2.5121824741363525, 2.4989914894104004, 2.6382083892822266, 2.4262871742248535, 2.4601569175720215, 2.5237534046173096, 2.4846482276916504, 2.491713285446167, 2.4572043418884277, 2.3300325870513916, 2.360365152359009, 2.757603645324707, 2.451246738433838, 2.6992833614349365, 2.68649959564209, 2.464505910873413, 2.4572339057922363, 2.4819788932800293, 2.467743396759033, 2.5794425010681152, 2.413726329803467, 2.3252370357513428, 2.4837417602539062, 2.576612949371338, 2.3841543197631836, 2.471372604370117, 2.5579042434692383, 2.552957057952881, 2.62308669090271, 2.4812850952148438, 2.6029326915740967, 2.4291133880615234, 2.565028429031372, 2.385113477706909, 2.41373872756958, 2.417336940765381, 2.2525475025177, 2.3563387393951416, 2.4421348571777344, 2.576261520385742, 2.435293197631836, 2.33040714263916, 2.442875623703003, 2.411609649658203, 2.5474557876586914, 2.6035804748535156, 2.4674770832061768, 2.5650298595428467, 2.318211078643799, 2.4174160957336426, 2.4982266426086426, 2.5440781116485596, 2.447669267654419, 2.478508949279785, 2.5730550289154053, 2.5185916423797607, 2.5788075923919678, 2.50312876701355, 2.3875772953033447, 2.5061652660369873, 2.5770676136016846, 2.572720766067505, 2.2818076610565186, 2.4107375144958496, 2.420552968978882, 2.4089863300323486, 2.38481068611145, 2.416804075241089, 2.3821310997009277, 2.6365835666656494, 2.3684449195861816, 2.433797836303711, 2.628236770629883, 2.4204118251800537, 2.4295096397399902, 2.6586313247680664, 2.3854055404663086, 2.5013773441314697, 2.423797130584717, 2.4433348178863525, 2.535137176513672, 2.4814953804016113, 2.4179956912994385, 2.4876034259796143, 2.449699878692627, 2.456982374191284, 2.4651036262512207, 2.3950281143188477, 2.593075752258301, 2.468414068222046, 2.4854202270507812, 2.4726414680480957, 2.476649761199951, 2.5194880962371826, 2.457610845565796, 2.440486192703247, 2.381683826446533, 2.4404845237731934, 2.418672800064087, 2.4351308345794678, 2.49271821975708, 2.4497873783111572, 2.4362971782684326, 2.5312490463256836, 2.482882261276245, 2.6308140754699707, 2.5084681510925293, 2.5108885765075684, 2.5222768783569336, 2.3882997035980225, 2.3903110027313232, 2.648869276046753, 2.3909144401550293, 2.476065158843994, 2.4635627269744873, 2.4966282844543457, 2.5076940059661865, 2.4142584800720215, 2.5205960273742676, 2.420774221420288, 2.4859228134155273, 2.391721725463867, 2.4581313133239746, 2.593069314956665, 2.3325140476226807, 2.431453227996826, 2.524360179901123, 2.4455204010009766, 2.394810199737549, 2.500011682510376, 2.424301862716675, 2.286836862564087, 2.5114798545837402, 2.45715594291687, 2.5336880683898926, 2.54569149017334, 2.4502203464508057, 2.620021343231201, 2.6573538780212402, 2.619413375854492, 2.399397850036621, 2.4512135982513428, 2.467585325241089, 2.387383222579956, 2.415266990661621, 2.4185664653778076, 2.4856884479522705, 2.5153090953826904, 2.449822425842285, 2.496725082397461, 2.523232936859131, 2.5215468406677246, 2.420886516571045, 2.4154839515686035, 2.447253704071045, 2.4130446910858154, 2.4533803462982178, 2.485715389251709, 2.4743282794952393, 2.5467193126678467, 2.620070457458496, 2.423283100128174, 2.626382827758789, 2.5961787700653076, 2.5129361152648926, 2.5702314376831055, 2.422022819519043, 2.368150472640991, 2.5539743900299072, 2.3331289291381836, 2.4669389724731445, 2.4832141399383545, 2.3894777297973633, 2.5581555366516113, 2.4398579597473145, 2.410518169403076, 2.4489905834198, 2.653104305267334, 2.4489591121673584, 2.4452829360961914, 2.4389467239379883, 2.5434939861297607, 2.434544801712036, 2.5360586643218994, 2.562621831893921, 2.4567551612854004, 2.528554677963257, 2.362131357192993, 2.522617816925049, 2.5283870697021484, 2.3286592960357666, 2.413282871246338, 2.417497396469116, 2.6039745807647705, 2.49521803855896, 2.613175868988037, 2.510385036468506, 2.438361644744873, 2.4323019981384277, 2.278304100036621, 2.5742273330688477, 2.414761543273926, 2.506338596343994, 2.3464982509613037, 2.644751787185669, 2.372877836227417, 2.400364637374878, 2.5408897399902344, 2.592071533203125, 2.5354552268981934, 2.545731544494629, 2.4850454330444336, 2.527194023132324, 2.4468185901641846, 2.496439218521118, 2.4019930362701416, 2.4387481212615967, 2.5631182193756104, 2.4626386165618896, 2.4375717639923096, 2.4037082195281982, 2.428740978240967, 2.4124093055725098, 2.6283071041107178, 2.47729229927063, 2.3859047889709473, 2.5038392543792725, 2.520594596862793, 2.5403223037719727, 2.498732328414917, 2.3771631717681885, 2.4088327884674072, 2.3675026893615723, 2.415328025817871, 2.4701104164123535, 2.4140872955322266, 2.375164031982422, 2.7107412815093994, 2.4304699897766113, 2.3115735054016113, 2.511298894882202, 2.5014381408691406, 2.475867509841919, 2.4132940769195557, 2.364341974258423, 2.609727382659912, 2.42390775680542, 2.45963191986084, 2.5385539531707764, 2.4407052993774414, 2.206389904022217, 2.639160394668579, 2.405503749847412, 2.372527599334717, 2.4907140731811523, 2.4379303455352783, 2.457648515701294, 2.305366277694702, 2.3582446575164795, 2.2960915565490723, 2.503354072570801, 2.496891736984253, 2.476036548614502, 2.519528865814209, 2.4930105209350586, 2.5424678325653076, 2.374969482421875, 2.388429880142212, 2.5061733722686768, 2.490640640258789, 2.544245481491089, 2.3901352882385254, 2.5245280265808105, 2.36523699760437, 2.467583656311035, 2.629507541656494, 2.4916510581970215, 2.4127964973449707, 2.389666795730591, 2.5030736923217773, 2.3580875396728516, 2.324434757232666, 2.368464946746826, 2.5453763008117676, 2.4747085571289062, 2.3858604431152344, 2.546851396560669, 2.4790613651275635, 2.3981685638427734, 2.3377318382263184, 2.3703808784484863, 2.629420518875122, 2.457064151763916, 2.3396642208099365, 2.4980580806732178, 2.516002893447876, 2.543020486831665, 2.488919496536255, 2.4700260162353516, 2.604217290878296, 2.4720358848571777, 2.3983523845672607, 2.5437638759613037, 2.4180185794830322, 2.445216178894043, 2.3991329669952393, 2.4200448989868164, 2.5811781883239746, 2.4466030597686768, 2.4141669273376465, 2.5543212890625, 2.6094565391540527, 2.407071352005005, 2.588953733444214, 2.5741937160491943, 2.485590934753418, 2.43294620513916, 2.3134992122650146, 2.3303894996643066, 2.500718593597412, 2.4321470260620117, 2.449618101119995, 2.546579360961914, 2.5701472759246826, 2.496762990951538, 2.2920475006103516, 2.5589232444763184, 2.4468841552734375, 2.4834585189819336, 2.3797128200531006, 2.4632668495178223, 2.576991081237793, 2.3333704471588135, 2.375483512878418, 2.5489771366119385, 2.476832389831543, 2.3432490825653076, 2.4385204315185547, 2.3386478424072266, 2.504812002182007, 2.4267308712005615, 2.406146287918091, 2.3550078868865967, 2.4861340522766113, 2.4409916400909424, 2.6732614040374756, 2.32957124710083, 2.4175732135772705, 2.503422498703003, 2.376906394958496, 2.4360415935516357, 2.5103931427001953, 2.471184492111206, 2.5560641288757324, 2.5455360412597656, 2.5184390544891357, 2.478011131286621, 2.465320348739624, 2.42171049118042, 2.459129571914673, 2.477531909942627, 2.515643358230591, 2.4361300468444824, 2.412222146987915, 2.338294267654419, 2.4910190105438232, 2.4412052631378174, 2.393867254257202, 2.3394694328308105, 2.4899425506591797, 2.4939119815826416, 2.5427324771881104, 2.5283379554748535, 2.3660356998443604, 2.4005231857299805, 2.326618194580078, 2.4531664848327637, 2.4187052249908447, 2.334989309310913, 2.586052894592285, 2.577990770339966, 2.373378276824951, 2.4690818786621094, 2.521010398864746, 2.537794589996338, 2.337296485900879, 2.444200038909912, 2.242105007171631, 2.3496105670928955, 2.366116523742676, 2.5045013427734375, 2.484326124191284, 2.4394960403442383, 2.4170496463775635, 2.3764748573303223, 2.4358816146850586, 2.4211130142211914, 2.565310478210449, 2.3322653770446777, 2.4497427940368652, 2.6493399143218994, 2.470654010772705, 2.570812225341797, 2.4331490993499756, 2.523277759552002, 2.4817452430725098, 2.4588370323181152, 2.5230929851531982, 2.4263482093811035, 2.3291780948638916, 2.390810966491699, 2.7092089653015137, 2.6576812267303467, 2.4458975791931152, 2.3348515033721924, 2.6038694381713867, 2.4589145183563232, 2.4936249256134033, 2.481255054473877, 2.4632606506347656, 2.311330795288086, 2.3709089756011963, 2.4261796474456787, 2.3969058990478516, 2.4480273723602295, 2.475691556930542, 2.3989665508270264, 2.3800010681152344, 2.4020349979400635, 2.6448466777801514, 2.5050337314605713, 2.5221481323242188, 2.4481253623962402, 2.4515228271484375, 2.5401103496551514, 2.3463776111602783, 2.469766855239868, 2.5905933380126953, 2.5586249828338623, 2.5463833808898926, 2.419142007827759, 2.4501218795776367, 2.55071759223938, 2.44814133644104, 2.392794370651245, 2.557054281234741, 2.4192330837249756, 2.3493335247039795, 2.462292194366455, 2.4739837646484375, 2.3568480014801025, 2.3185629844665527, 2.311859130859375, 2.5096373558044434, 2.4015321731567383, 2.4635579586029053, 2.464975118637085, 2.4772112369537354, 2.3601436614990234, 2.492579221725464, 2.366480827331543, 2.3475396633148193, 2.448352098464966, 2.5190353393554688, 2.455221176147461, 2.504998207092285, 2.624241590499878, 2.4168760776519775, 2.390929937362671, 2.2739288806915283, 2.6038577556610107, 2.395387887954712, 2.4493799209594727, 2.3528389930725098, 2.600935459136963, 2.3624162673950195, 2.409928321838379, 2.463304042816162, 2.3298211097717285, 2.461430788040161, 2.475602865219116, 2.580172538757324, 2.4921162128448486, 2.496269941329956, 2.446179151535034, 2.477438449859619, 2.393728017807007, 2.496638774871826, 2.4023070335388184, 2.271040678024292, 2.6610755920410156, 2.5626485347747803, 2.354851722717285, 2.5363447666168213, 2.5340018272399902, 2.399137496948242, 2.5806570053100586, 2.3482701778411865, 2.498108148574829, 2.3808984756469727, 2.2824478149414062, 2.460026264190674, 2.5735034942626953, 2.3735508918762207, 2.5816850662231445, 2.3975367546081543, 2.377042293548584, 2.407101631164551, 2.440586805343628, 2.5127108097076416, 2.5089473724365234, 2.552485227584839, 2.5884597301483154, 2.4017462730407715, 2.3067996501922607, 2.4872615337371826, 2.3450374603271484, 2.5259881019592285, 2.5935680866241455, 2.4558353424072266, 2.5826802253723145, 2.420086622238159, 2.3364338874816895, 2.392284870147705, 2.61122989654541, 2.5244600772857666, 2.446655511856079, 2.4936327934265137, 2.5138564109802246, 2.5256383419036865, 2.4511420726776123, 2.3502049446105957, 2.5446319580078125, 2.5530591011047363, 2.521530866622925, 2.4827780723571777, 2.4001071453094482, 2.5811896324157715, 2.478146553039551, 2.330669641494751, 2.3374698162078857, 2.5107839107513428, 2.3933377265930176, 2.426548719406128, 2.4303338527679443, 2.4531400203704834, 2.5912411212921143, 2.5948712825775146, 2.6634573936462402, 2.5610945224761963, 2.504772901535034, 2.4165122509002686, 2.435750722885132, 2.555433988571167, 2.37487530708313, 2.5223889350891113, 2.5356404781341553, 2.5209388732910156, 2.3745534420013428, 2.53126859664917, 2.4933598041534424, 2.366577386856079, 2.4982552528381348, 2.511612892150879, 2.5118281841278076, 2.415241241455078, 2.563774824142456, 2.529121160507202, 2.4406979084014893, 2.6539103984832764, 2.50354266166687, 2.441934823989868, 2.438974618911743, 2.623349189758301, 2.3993070125579834, 2.4654338359832764, 2.434807062149048, 2.5486536026000977, 2.49656081199646, 2.434764862060547, 2.578651189804077, 2.4396378993988037, 2.561643600463867, 2.334108829498291, 2.506753921508789, 2.4600188732147217, 2.499838352203369, 2.3962056636810303, 2.4929680824279785, 2.3831727504730225, 2.5561020374298096, 2.5474417209625244, 2.4596121311187744, 2.3998923301696777, 2.5464417934417725, 2.6083953380584717, 2.4522054195404053, 2.456897020339966, 2.658817768096924, 2.6150472164154053, 2.475337266921997, 2.334973096847534, 2.3896586894989014, 2.4210846424102783, 2.302515745162964, 2.338951826095581, 2.312530040740967, 2.5628607273101807, 2.5124549865722656, 2.396862506866455, 2.601292848587036, 2.335719108581543, 2.447194814682007, 2.5337839126586914, 2.3291144371032715, 2.5062267780303955, 2.3270227909088135, 2.389580011367798, 2.4473955631256104, 2.3693554401397705, 2.5406134128570557, 2.3934788703918457, 2.3836424350738525, 2.5450899600982666, 2.271132469177246, 2.381739616394043, 2.4835398197174072, 2.4109947681427, 2.5678868293762207, 2.5444176197052, 2.5072250366210938, 2.3129284381866455, 2.3333330154418945, 2.4873874187469482, 2.5135300159454346, 2.4164764881134033, 2.558434247970581, 2.644404649734497, 2.575011968612671, 2.5275096893310547, 2.594932794570923, 2.420164108276367, 2.418009042739868, 2.527374029159546, 2.467183828353882, 2.4433226585388184, 2.5776429176330566, 2.5270602703094482, 2.372921943664551, 2.360846996307373, 2.458833694458008, 2.455409288406372, 2.313241481781006, 2.43599796295166, 2.5827653408050537, 2.4579877853393555, 2.481971502304077, 2.4497244358062744, 2.518580913543701, 2.4923110008239746, 2.5396175384521484, 2.552765130996704, 2.63997483253479, 2.633646011352539, 2.4974005222320557, 2.5297415256500244, 2.388498544692993, 2.3108396530151367, 2.354776620864868, 2.365577459335327, 2.28165864944458, 2.462442398071289, 2.3045780658721924, 2.4564929008483887, 2.5473644733428955, 2.3905904293060303, 2.360006809234619, 2.5834007263183594, 2.3587605953216553, 2.483930826187134, 2.3791651725769043, 2.6323070526123047, 2.3241865634918213, 2.490398645401001, 2.394092559814453, 2.5733535289764404, 2.4439547061920166, 2.3445568084716797, 2.4063098430633545, 2.3799502849578857, 2.4103901386260986, 2.4444780349731445, 2.608720064163208, 2.468602180480957, 2.4731814861297607, 2.3146512508392334, 2.443452835083008, 2.471726655960083, 2.3831663131713867, 2.4856441020965576, 2.3822922706604004, 2.4790079593658447, 2.4122660160064697, 2.417880058288574, 2.5277011394500732, 2.308306932449341, 2.3165881633758545, 2.395749568939209, 2.3940207958221436, 2.5729222297668457, 2.4540250301361084, 2.330798387527466, 2.38795804977417, 2.5132858753204346, 2.4632978439331055, 2.485034227371216, 2.396709442138672, 2.406677007675171, 2.4666852951049805, 2.418121099472046, 2.326967477798462, 2.5890142917633057, 2.524608850479126, 2.553036689758301, 2.3918263912200928, 2.5483062267303467, 2.5181639194488525, 2.528822660446167, 2.537489891052246, 2.480276584625244, 2.2904319763183594, 2.4886598587036133, 2.706777572631836, 2.5374720096588135, 2.4219624996185303, 2.5080604553222656, 2.469365119934082, 2.451129198074341, 2.590458393096924, 2.49281644821167, 2.4881367683410645, 2.5059776306152344, 2.381187915802002, 2.382633686065674, 2.3621222972869873, 2.3451220989227295, 2.406439781188965, 2.478759527206421, 2.550046920776367, 2.2505617141723633, 2.439504384994507, 2.481271266937256, 2.404104709625244, 2.549333333969116, 2.3584702014923096, 2.4363434314727783, 2.507425308227539, 2.6018619537353516, 2.505816698074341, 2.3606414794921875, 2.4473624229431152, 2.429168462753296, 2.4843332767486572, 2.510192632675171, 2.5735278129577637, 2.6207833290100098, 2.44863224029541, 2.374069929122925, 2.5231997966766357, 2.5513901710510254, 2.5355606079101562, 2.3099775314331055, 2.3796703815460205, 2.591670513153076, 2.5673787593841553, 2.4544260501861572, 2.529395580291748, 2.5548839569091797, 2.4555423259735107, 2.3965494632720947, 2.4819581508636475, 2.5306830406188965, 2.4270386695861816, 2.507676839828491, 2.3374593257904053, 2.5520641803741455, 2.428105592727661, 2.4859702587127686, 2.4508397579193115, 2.3597054481506348, 2.417466640472412, 2.620255470275879, 2.2982723712921143, 2.404264211654663, 2.33640193939209, 2.4641261100769043, 2.4939863681793213, 2.39766788482666, 2.4194610118865967, 2.599592685699463, 2.589529037475586, 2.3771793842315674, 2.50034761428833, 2.4866137504577637, 2.4340054988861084, 2.461251974105835, 2.457794189453125, 2.3551201820373535, 2.427964925765991, 2.354940176010132, 2.5479812622070312, 2.499591827392578, 2.6818034648895264, 2.4466114044189453, 2.5934979915618896, 2.4626083374023438, 2.3557584285736084, 2.5470199584960938, 2.553816080093384, 2.651693820953369, 2.3851072788238525, 2.4094526767730713, 2.222052574157715, 2.4621469974517822, 2.4366543292999268, 2.45709228515625, 2.6704790592193604, 2.37859845161438, 2.4273858070373535, 2.3769302368164062, 2.470998764038086, 2.425910234451294, 2.471952438354492, 2.453321695327759, 2.4672014713287354, 2.5249764919281006, 2.3860268592834473, 2.458881378173828, 2.3632850646972656, 2.617797374725342, 2.328019857406616, 2.7233407497406006, 2.45772385597229, 2.3546042442321777, 2.512334108352661, 2.3950846195220947, 2.3863236904144287, 2.481893301010132, 2.454301357269287, 2.6097493171691895, 2.560408115386963, 2.551920175552368, 2.633634567260742, 2.53761625289917, 2.3394274711608887, 2.501533269882202, 2.5401198863983154, 2.421983242034912, 2.5334980487823486, 2.511744260787964, 2.610511064529419, 2.5407907962799072, 2.2349555492401123, 2.3671023845672607, 2.3467206954956055, 2.5804028511047363, 2.599090337753296, 2.3992903232574463, 2.6000547409057617, 2.4230153560638428, 2.4091882705688477, 2.563750743865967, 2.58341383934021, 2.5451138019561768, 2.375519275665283, 2.2495908737182617, 2.5119245052337646, 2.6033718585968018, 2.3885152339935303, 2.611480712890625, 2.5178303718566895, 2.4617927074432373, 2.4805617332458496, 2.4466350078582764, 2.4100582599639893, 2.3984570503234863, 2.4778006076812744, 2.3994905948638916, 2.5523619651794434, 2.424861192703247, 2.3531124591827393, 2.597224235534668, 2.3771650791168213, 2.403397560119629, 2.6302082538604736, 2.4348108768463135, 2.520449638366699, 2.4284775257110596, 2.6262364387512207, 2.6480090618133545, 2.3868136405944824, 2.5233821868896484, 2.391566514968872, 2.4271183013916016, 2.4437596797943115, 2.3638718128204346, 2.355513095855713, 2.440937042236328, 2.576260566711426, 2.4646542072296143, 2.4839537143707275, 2.416684150695801, 2.399672269821167, 2.402766227722168, 2.3207225799560547, 2.5739939212799072, 2.4500930309295654, 2.529197931289673, 2.510887622833252, 2.417354106903076, 2.477611780166626, 2.5458426475524902, 2.3830199241638184, 2.458040952682495, 2.3416435718536377, 2.3970165252685547, 2.2587814331054688, 2.2372279167175293, 2.449260950088501, 2.5207881927490234, 2.3148138523101807, 2.5447723865509033, 2.482696771621704, 2.4974143505096436, 2.3862767219543457, 2.5074093341827393, 2.5416922569274902, 2.4910242557525635, 2.3587920665740967, 2.4045445919036865, 2.5769665241241455, 2.4517998695373535, 2.5421533584594727, 2.450122117996216, 2.4964487552642822, 2.376490354537964, 2.4099514484405518, 2.332138776779175, 2.3918979167938232, 2.430783271789551, 2.6326539516448975, 2.5136828422546387, 2.509016275405884, 2.350271224975586, 2.4225881099700928, 2.4247171878814697, 2.4965782165527344, 2.4195287227630615, 2.4536828994750977, 2.4126734733581543, 2.4161489009857178, 2.451355218887329, 2.354893445968628, 2.380554437637329, 2.4454119205474854, 2.357475519180298, 2.365176200866699, 2.2867796421051025, 2.4816534519195557, 2.384195566177368, 2.4182987213134766, 2.3757975101470947, 2.3294577598571777, 2.2958970069885254, 2.404472827911377, 2.4832465648651123, 2.372859239578247, 2.512218475341797, 2.3988430500030518, 2.512972116470337, 2.4831643104553223, 2.495889663696289, 2.3962042331695557, 2.575787305831909, 2.2488362789154053, 2.41487193107605, 2.3416261672973633, 2.3434572219848633, 2.432684898376465, 2.398674488067627, 2.2828776836395264, 2.6202757358551025, 2.4214580059051514, 2.6029462814331055, 2.2888307571411133, 2.567866563796997, 2.5410242080688477, 2.5837979316711426, 2.2452685832977295, 2.508272647857666, 2.5793814659118652, 2.41373610496521, 2.549935817718506, 2.390630006790161, 2.3570923805236816, 2.3803212642669678, 2.3207507133483887, 2.338488817214966, 2.4155168533325195, 2.390791893005371, 2.4301042556762695, 2.274178981781006, 2.388312339782715, 2.451378345489502, 2.33728289604187, 2.5845677852630615, 2.170428514480591, 2.3496203422546387, 2.642491579055786, 2.302659034729004, 2.4619534015655518, 2.4291162490844727, 2.499905586242676, 2.349407911300659, 2.5105197429656982, 2.5581934452056885, 2.40706729888916, 2.5127980709075928, 2.3571786880493164, 2.456120729446411, 2.4932892322540283, 2.407019853591919, 2.4332523345947266, 2.630589485168457, 2.6032140254974365, 2.403639793395996, 2.4464614391326904, 2.5739502906799316, 2.539527416229248, 2.4781112670898438, 2.4614076614379883, 2.3494906425476074, 2.315629720687866, 2.4082038402557373, 2.4649581909179688, 2.477719783782959, 2.4529314041137695, 2.602771043777466, 2.500948429107666, 2.5838210582733154, 2.5311954021453857, 2.2860896587371826, 2.512669086456299, 2.5019216537475586, 2.229823589324951, 2.320544958114624, 2.210000991821289, 2.4411423206329346, 2.525923490524292, 2.4593448638916016, 2.3864634037017822, 2.553570032119751, 2.4045791625976562, 2.446535110473633, 2.401128053665161, 2.534130811691284, 2.3929550647735596, 2.3932011127471924, 2.670438051223755, 2.3393161296844482, 2.444815158843994, 2.458798408508301, 2.4934117794036865, 2.5842528343200684, 2.5373897552490234, 2.301689863204956, 2.5745177268981934, 2.4542033672332764, 2.4688096046447754, 2.401118755340576, 2.4694509506225586, 2.402231454849243, 2.4743943214416504, 2.47121524810791, 2.5046093463897705, 2.3435139656066895, 2.337926149368286, 2.2807729244232178, 2.5499448776245117, 2.4612650871276855, 2.4834389686584473, 2.497680425643921, 2.5238919258117676, 2.4532666206359863, 2.3946075439453125, 2.3755269050598145, 2.48018741607666, 2.405513048171997, 2.4074184894561768, 2.345637798309326, 2.58683180809021, 2.448246955871582, 2.395533561706543, 2.501559257507324, 2.316821813583374, 2.5128986835479736, 2.685241460800171, 2.4171111583709717, 2.3255598545074463, 2.4603874683380127, 2.2363197803497314, 2.5914289951324463, 2.4352591037750244, 2.323655366897583, 2.529960870742798, 2.516526699066162, 2.4339728355407715, 2.482042074203491, 2.476304769515991, 2.4017674922943115, 2.408205509185791, 2.494471311569214, 2.363506555557251, 2.3541924953460693, 2.4926917552948, 2.4728267192840576, 2.538719415664673, 2.467961311340332, 2.490354537963867, 2.489241123199463, 2.5788378715515137, 2.3459866046905518, 2.461627244949341, 2.4436285495758057, 2.3892650604248047, 2.523418664932251, 2.4384212493896484, 2.5109710693359375, 2.649247646331787, 2.329224109649658, 2.5936965942382812, 2.494476079940796, 2.5292584896087646, 2.4304757118225098, 2.544389009475708, 2.466351270675659, 2.4182145595550537, 2.341871976852417, 2.523716449737549, 2.4386982917785645, 2.4845850467681885, 2.400831460952759, 2.5136759281158447, 2.51727557182312, 2.4662368297576904, 2.5285332202911377, 2.459879159927368, 2.2982726097106934, 2.511155366897583, 2.3734889030456543, 2.4682259559631348, 2.333768606185913, 2.6363584995269775, 2.4114291667938232, 2.640115976333618, 2.567793369293213, 2.41930890083313, 2.3917951583862305, 2.461203098297119, 2.4111993312835693, 2.443424701690674, 2.455826759338379, 2.504894256591797, 2.4254229068756104, 2.576753616333008, 2.4083778858184814, 2.4615681171417236, 2.443025827407837, 2.411471366882324, 2.5798871517181396, 2.349322557449341, 2.4737091064453125, 2.4683144092559814, 2.4541056156158447, 2.3417000770568848, 2.3890745639801025, 2.3494157791137695, 2.4460065364837646, 2.3264527320861816, 2.3368968963623047, 2.4552624225616455, 2.3316900730133057, 2.2911927700042725, 2.522639513015747, 2.5231382846832275, 2.425200939178467, 2.480111598968506, 2.640573740005493, 2.6125552654266357, 2.435157299041748, 2.3430423736572266, 2.5256552696228027, 2.458691120147705, 2.4230501651763916, 2.6888604164123535, 2.3724770545959473, 2.4841039180755615, 2.4254796504974365, 2.484308958053589, 2.527238368988037, 2.463120222091675, 2.4919064044952393, 2.3845808506011963, 2.4451820850372314, 2.4686827659606934, 2.5822813510894775, 2.497013568878174, 2.374316930770874, 2.36791729927063, 2.464298963546753, 2.5779385566711426, 2.381312131881714, 2.433427572250366, 2.443744421005249, 2.606004238128662, 2.3220412731170654, 2.4761674404144287, 2.3735010623931885, 2.5155715942382812, 2.4151384830474854, 2.478990077972412, 2.554594039916992, 2.4753715991973877, 2.6430344581604004, 2.343153953552246, 2.3747055530548096, 2.4185738563537598, 2.4363181591033936, 2.5101873874664307, 2.4096877574920654, 2.4943716526031494, 2.280073404312134, 2.5602715015411377, 2.480539321899414, 2.413846731185913, 2.374974489212036, 2.542710542678833, 2.392542839050293, 2.514453411102295, 2.3675458431243896, 2.6474192142486572, 2.4720358848571777, 2.371716022491455, 2.582667350769043, 2.4536943435668945, 2.498563289642334, 2.470916509628296, 2.597348928451538, 2.573777914047241, 2.431734561920166, 2.574270009994507, 2.3708131313323975, 2.6146788597106934, 2.3811635971069336, 2.636091709136963, 2.543062210083008, 2.5736083984375, 2.406097888946533, 2.452580451965332, 2.450883626937866, 2.426790714263916, 2.199070692062378, 2.4348487854003906, 2.5896849632263184, 2.356055736541748, 2.35168194770813, 2.418301820755005, 2.459521532058716, 2.5350441932678223, 2.4683401584625244, 2.474030017852783, 2.4645025730133057, 2.337674379348755, 2.4868879318237305, 2.4693946838378906, 2.4598631858825684, 2.3591694831848145, 2.4237470626831055, 2.4849071502685547, 2.372610092163086, 2.4426169395446777, 2.4937994480133057, 2.5156874656677246, 2.5143990516662598, 2.4387409687042236, 2.320524215698242, 2.5038464069366455, 2.452225685119629, 2.5448741912841797, 2.580754280090332, 2.5727427005767822, 2.416978359222412, 2.345484733581543, 2.560647487640381, 2.494213819503784, 2.334416151046753, 2.475412130355835, 2.3583576679229736, 2.3419551849365234, 2.4332499504089355, 2.340602159500122, 2.3060243129730225, 2.4900968074798584, 2.386500835418701, 2.587686061859131, 2.437664031982422, 2.3659114837646484, 2.325427770614624, 2.5113015174865723, 2.2813918590545654, 2.3327670097351074, 2.38680100440979, 2.4869914054870605, 2.4596712589263916, 2.494342803955078, 2.478790521621704, 2.479814291000366, 2.4192490577697754, 2.428509473800659, 2.4169225692749023, 2.5594115257263184, 2.5381457805633545, 2.456064224243164, 2.382483959197998, 2.5349230766296387, 2.4676690101623535, 2.39481520652771, 2.551997661590576, 2.3180806636810303, 2.539285898208618, 2.6247336864471436, 2.473961114883423, 2.354499101638794, 2.56799578666687, 2.481386184692383, 2.4879343509674072]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = torch.load('lossPlutchikCNN')\n",
    "print(loss)\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.51266610622406, 1.4683917760849, 1.4606459140777588]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('lossMaslowBERT', 'rb')\n",
    "loss_maslow_bert = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 3], [1, 5], [1, 6]])\n",
    "b = np.array([[2, 5], [2, 3], [2, 2]])\n",
    "\n",
    "a = np.apply_along_axis(predict_one_hot, axis=1, arr=b)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     0.     0.     0.     0.     0.     0.     0.  2558. 17928.\n",
      " 22770.     0.     0.     0.     0.     0.     0.]\n",
      "[    0.     0.     0.     0.     0.     0.     0.     0. 22770. 22770.\n",
      " 22770.     0.     0.     0.     0.     0.     0.]\n"
     ]
    }
   ],
   "source": [
    "c = np.load('test_run_2/labels_bert_maslow.npy')\n",
    "print(np.sum(c, axis=0))\n",
    "d = np.load('test_run_2/preds_bert_maslow.npy')\n",
    "print(np.sum(d, axis=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
