{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tokens import FullTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(input_string: str) -> list:\n",
    "    res = input_string.strip('][')\n",
    "    res = res.replace('\"', '')\n",
    "    res = res.split(', ')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file, bert_layer, only_a_few=False):\n",
    "    file = file[file['action'] != 'no']\n",
    "    file = file[file['motivation'] != 'none']\n",
    "    file = file[file['maslow'] != '[]']\n",
    "    file = file[file['reiss'] != '[]']\n",
    "    file = file[file['reiss'] != '[\"na\"]']\n",
    "    file[\"maslow\"] = file[\"maslow\"].apply(string_to_list)\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(file[\"maslow\"])\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_motivation = pd.read_csv(os.path.join(\"scs-baselines-master/data/dev/motivation\", \"allcharlinepairs_noids.csv\"))\n",
    "testing_file_motivation = pd.read_csv(os.path.join(\"scs-baselines-master/data/test/motivation\", \"allcharlinepairs_noids.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 20:54:01.678613: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=128, only_a_few=False):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    j = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "        j += 1\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(5, activation='sigmoid')(net)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "broken\n"
     ]
    }
   ],
   "source": [
    "train_input, train_labels = preprocess(training_file_motivation, bert_layer)\n",
    "test_input, test_labels = preprocess(testing_file_motivation, bert_layer, only_a_few=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
      "                                 (None, 128, 768)]                'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['keras_layer[0][1]']            \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           49216       ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            165         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,533,702\n",
      "Trainable params: 109,533,701\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/718 [=========>....................] - ETA: 12:17:17 - loss: 2.0168 - accuracy: 0.3302"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, \n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03188419 0.08617873 0.80413693 0.02892018 0.04888003]\n",
      " [0.03188419 0.08617873 0.80413693 0.02892018 0.04888003]\n",
      " [0.03188419 0.08617873 0.80413693 0.02892018 0.04888003]\n",
      " [0.28705162 0.05856194 0.02321398 0.14558955 0.48558298]\n",
      " [0.28705162 0.05856194 0.02321398 0.14558955 0.48558298]\n",
      " [0.54748553 0.1383801  0.01152629 0.23038761 0.07222055]\n",
      " [0.02476677 0.05374946 0.73146236 0.02509226 0.1649291 ]\n",
      " [0.38043195 0.31352013 0.04867551 0.24522965 0.01214272]\n",
      " [0.38043195 0.31352013 0.04867551 0.24522965 0.01214272]\n",
      " [0.6074848  0.14596707 0.00161824 0.24225476 0.00267522]\n",
      " [0.6074848  0.14596707 0.00161824 0.24225476 0.00267522]\n",
      " [0.3344316  0.03081152 0.01772548 0.2688434  0.34818804]\n",
      " [0.3344316  0.03081152 0.01772548 0.2688434  0.34818804]\n",
      " [0.17419957 0.01192915 0.02532709 0.06031655 0.7282276 ]\n",
      " [0.4241949  0.02954231 0.01432405 0.13679944 0.39513925]\n",
      " [0.4241949  0.02954231 0.01432405 0.13679944 0.39513925]\n",
      " [0.25527433 0.4360069  0.00837377 0.17605321 0.12429176]\n",
      " [0.25527433 0.4360069  0.00837377 0.17605321 0.12429176]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.03771479 0.9333943  0.00245156 0.01526314 0.01117608]\n",
      " [0.11249669 0.20095865 0.0396274  0.21135764 0.43555966]\n",
      " [0.11249669 0.20095865 0.0396274  0.21135764 0.43555966]\n",
      " [0.14031023 0.15072371 0.06413933 0.13397785 0.5108489 ]\n",
      " [0.1407514  0.70310533 0.01860071 0.1360024  0.00154014]\n",
      " [0.1407514  0.70310533 0.01860071 0.1360024  0.00154014]\n",
      " [0.1407514  0.70310533 0.01860071 0.1360024  0.00154014]]\n",
      "[[0 0 1 0 0]\n",
      " [1 1 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " ...\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test_output = model.predict(test_input)\n",
    "print(test_output)\n",
    "print(test_labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
